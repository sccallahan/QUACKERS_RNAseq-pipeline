---
title: "QUACKERS (an RNAseq pipeline)"
author: "Carson Callahan"
date: "July 2, 2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Intro

![Desperate times, desperate measures.](http://static.makeuseof.com/wp-content/uploads/2017/12/weird-programming-principles-670x335.jpg "Desperate times, desperate measures.")



The pipelines our lab typically uses for NGS have recently been broken by changes to our HPC. While running the tools in a standalone fashion isn't particularly complicated and ultimately results in the same output, allowing users to run the entire workflow using by supplying arguments to a single script is generally more convenient and allows less experienced users to run their workflows unassisted... plus it saves some time on the user end by not needing to babysit individual jobs.

With this in mind, I've created a small, simple pipline called QUACKERS - **QU**ickly **A**ssembled **C**ode that **K**nits **E**ssential **R**NAseq **S**cripts (though it's less "quickly assembled" as time goes on...). In short, users supply arguments to a parent script, which then passes these arguments to child scripts that submit jobs to the cluster. Currently, the workflow will align fastqs using STAR, generate a multiqc report using fastqc outputs (from bams) and STAR outputs, and generate a counts matrix using featureCounts and [some custom bits of code from Ming (Tommy) Tang.](https://divingintogeneticsandgenomics.rbind.io/post/merge-featurecount-table-from-rnaseq/)

## Setting up the tools

This pipeline relies on STAR for alignments and featureCounts for counting aligned reads. Both of these tools require external files, the paths of which must be provided (covered later in Running the Code). In particular, you need the .gtf files and index generated from STAR and a .gtf file for featureCounts (this should generally be the same as the one used to generate the STAR index).

## Distribute workflow

```{bash eval=FALSE}

# connect to Shark
ssh username@shark.mdanderson.edu # alternatively, `ssh shark` if on a hard connection

# make a folder to hold all the files - name it whatever you like
mkdir workdir
cd workdir

# clone the repo
source activate py351 # need this for `git clone` and `multiqc` later
git clone https://github.com/sccallahan/QUACKERS_RNAseq-pipeline
cd QUACKERS_RNAseq-pipeline
```

This should deposit all the scripts into your directory. Next, you need to create a sample sheet. This sheet MUST BE NAMED `sample_sheet_STAR.txt`, or the pipeline will fail. Alternatively, you could edit the `quack_STAR.sh` file to whatever you named your sample sheet. The sample sheet needs to be a tabbed texted file, with each paired end fastq belonging to the same row and separate columns. Here's an example:

```{bash eval=FALSE}
HN30_KD1_1.fq.gz	HN30_KD1_2.fq.gz
HN30_KD2_1.fq.gz	HN30_KD2_2.fq.gz
HN30_KD3_1.fq.gz	HN30_KD3_2.fq.gz
HN30_NT1_1.fq.gz	HN30_NT1_2.fq.gz
HN30_NT2_1.fq.gz	HN30_NT2_2.fq.gz
HN30_NT3_1.fq.gz	HN30_NT3_2.fq.gz
```

You should now have the following files in the same (working) directory:

* `beginQuacking.sh`
* `quack_STAR.sh`
* `quack_fastqc.sh`
* `quack_multiqc.sh`
* `quack_mergeCounts.sh`
* `sample_sheet_STAR.txt`

It's also important that your fastq files are all in the same directory.

**NB:** The pipeline cannot currently recursively search folders, i.e., you'll need to move each fastq into the same parent directory (some cores provide fastq.gz directly, some give you each folder containing each pair of fastq.gz as they come off the sequencing machine). I plan on adding this functionality when I get some time to work on it. In the mean time, I've uploaded a hack-y workaround called `extract_fastq_from_folder.sh`. Place this script in your fastq directory and run it - it will move into each folder and rsync the fastq into the parent directory, provided your directory structure is:

* Fastq_dir
    * Fastq_sample_1_dir
        * Fastq_files
    * Fastq_sample_2_dir
        * Fastq_files

and so on. After the pipeline is done, you can freely delete the rsync'd fastqs to save disk space if you'd like.

Additionally, you can modify the `quack_STAR.sh` file to drop the fastq suffix from your sample names in future outputs. This option can be found on line 27 of the script. Simply change '.fastq.gz' to whatever your fastq suffix is.

## Running the Code

Simply run the parent script `beginQuacking.sh` In brief, this script is run with the following syntax:

`bash beginQuacking.sh [fastq directory] [index path] [star gtf file] [fc gtf file] [sleep time]`

Here's a brief explanation of the 5 arguments:

* fastq directory - a folder containing only the fastqs of interest
* index path - the *folder* containing the STAR index
* star gtf file - path to the gtf file(s) used to generate the STAR index, or the gtfs to be used on the fly during mapping. Can take a syntax like `/path/to/files/*.gtf` as well.
* fc gtf file - path to the gtf file to be used for featureCounts. This should generally be the same as the star tf file.
* sleep time - amount of time to wait before checking for files during aligment, counting, etc. For example, 60 would tell the script to check for completion of alignment every 60 seconds and print a message to the terminal. In most cases, 60-180 seconds is probably best.

So, running the script for me would look something like this:

```{bash eval=FALSE}
bash beginQuacking.sh /rsrch2/headneck/sccallahan/testdir/testfqs /rsrch2/headneck/sccallahan/apps/annot/starbase/hg19/starindex /rsrch2/headneck/sccallahan/apps/annot/starbase/hg19/annot/*.gtf /rsrch2/headneck/sccallahan/apps/annot/starbase/hg19/annot/*.gtf 60
```

This will submit all the jobs. The scripts will make folders in the current workding directory and place relevant outputs there. The created folders are:

* logs - has .out and .err logs
    * lsf_logs - the files containing the job submission scripts
* fastqc_output - fastqc outputs
* multiqc_output - multiqc outputs
* STAR_output - STAR outputs
* featureCounts_output - raw counts files for each sample, as well as a merged table with the counts for all samples.

## Job control

The main control of interest is killing the jobs. This can be done as follows:

```{bash eval=FALSE}
bkill ` bjobs -u sccallahan |grep RUN |cut -f1 -d" "` # replace my username with yours, can also change RUN to PEND or relevant status
```

## Downstream processing

For downstream work, you should first check the multiqc output to make sure your sequencing was of sufficient quality (uniquely mapped reads, duplication rates, etc.).

Next, read the raw counts table into R. You'll notice the column names are very messy, but this can be easy cleaned up by manually setting column names, or being clever with regex to do it for you. I don't have too many samples, so it's easy to set manually.

```{r eval=FALSE}
library(tidyverse)

counts <- read.table("/path/to/raw_counts_table.txt", header = TRUE)
counts <- column_to_rownames(counts, var = "Geneid")
colnames(counts) <- c("KD1", "KD2", "KD3", "NT1", "NT2", "NT3") # you could also try some regex here, which is useful if you have tons of samples
```

Now you should have a counts matrix that is ready for analysis with DESeq2 or other tools.







